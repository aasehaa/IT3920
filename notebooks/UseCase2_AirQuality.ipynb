{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Use Case 2: Air Quality Monitoring üå¨Ô∏è\n",
        "\n",
        "*The provided notebook is inspired by Open Sourced City Scanner, a project developed at Senseable City Lab. More information about this project can be found [here](https://github.com/MIT-Senseable-City-Lab/OSCS/tree/main/Learn/Coding%20Exercise).*\n",
        "\n",
        "This notebook will guide you through how to analyze air quality data on a map from the octopus.\n",
        "\n",
        "## Initial Setup\n",
        "\n",
        "In the beginning of this setup, you will connect to your google drive and download libraries needed for analysis. Then, you will need to clean your data to make it ready to plot in a map."
      ],
      "metadata": {
        "id": "6tU7wl1tUYEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Block 1\n",
        "#this block is going to allow the notebook to connect to your google drive so you can interact with it\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uV-MK44j0Zjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Block 2\n",
        "#here, we install the tool we are going to use to make some maps, called Folium\n",
        "%%capture\n",
        "!pip install folium"
      ],
      "metadata": {
        "id": "lvb0dTp7X3Em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Block 3\n",
        "#this block of code is where we get the infrastructure for the notebook set up, by calling libraries\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "#these libraries will help us read in and format the data correctly\n",
        "import pytz\n",
        "import time\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "#these libraries will help us with our time series analysis\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import ticker as mticker\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "#these libraries will support the mapping work\n",
        "import folium\n",
        "from folium import plugins\n",
        "import branca.colormap as cm\n",
        "from matplotlib.dates import DateFormatter\n",
        "from sklearn.cluster import DBSCAN\n",
        "from geopy.distance import great_circle\n",
        "from shapely.geometry import MultiPoint"
      ],
      "metadata": {
        "id": "TyYM8PuW0gLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Block 4\n",
        "#adjust the path below to match your google drive setup! please use the format below:\n",
        "os.chdir('/content/drive/My Drive/path to correct folder/')"
      ],
      "metadata": {
        "id": "5CrJeuMl0maA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 5\n",
        "# Read the raw data from a CSV file into a DataFrame\n",
        "# adjust name to correct file\n",
        "filename = \"octopus_data_test.txt\"\n",
        "df = pd.read_csv(filename, names=[\"timestamp\", \"temperature\", \"humidity\", \"pressure\", \"pm1\", \"pm4\", 'pm25', \"pm10\"])\n",
        "\n",
        "# Display the first values in the dataframe\n",
        "df.head()\n",
        "\n",
        "# Display the last values in the dataframe\n",
        "df.tail()"
      ],
      "metadata": {
        "id": "2zed2sL1tPzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 - Data Preprocessing\n",
        "Depending on you use case and what you want to answer with your data, different pre-processing steps should be done to get the best results possible. In this case, we will conduct Handling Missing Values, Outlier Detection and Removal, Resampling, Normalization, Smoothing and Seasonal Decomposition."
      ],
      "metadata": {
        "id": "_XST0eILswjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 5\n",
        "# Handling missing values\n",
        "# Handling missing values for multiple columns using forward fill\n",
        "columns_to_fill = [\"temperature\", \"humidity\", \"pressure\", \"pm1\", \"pm4\", \"pm25\", \"pm10\"]\n",
        "for column in columns_to_fill:\n",
        "    df[column].fillna(method='ffill', inplace=True)\n"
      ],
      "metadata": {
        "id": "JKKnDLer0-Iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 6\n",
        "# Outlier detection and removal (using Z-score)\n",
        "columns_to_check = [\"temperature\", \"humidity\", \"pressure\", \"pm1\", \"pm4\", \"pm25\", \"pm10\"]\n",
        "z_threshold = 3\n",
        "\n",
        "for column in columns_to_check:\n",
        "    df[column + '_z'] = (df[column] - df[column].mean()) / df[column].std()\n",
        "\n",
        "condition = np.logical_and.reduce([(df[col + '_z'].abs() < z_threshold) for col in columns_to_check])\n",
        "df = df[condition]\n",
        "\n",
        "# Optionally, drop the intermediate Z-score columns if they are no longer needed\n",
        "df.drop([col + '_z' for col in columns_to_check], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "FnfSZsof2e6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 7\n",
        "# Resampling\n",
        "# This part will depend on the frequency of your collected data, and what you want to analyse.\n",
        "# In this example, the data is collected every second, but we want to analyze the mean values every 15 min\n",
        "# '15T' specifies the new frequency, indicating every 15 minutes. You can change to '1H' if you want every hour.\n",
        "# .mean() specifies the aggregation method, meaning that within each new 15-minute interval, the mean of the values will be calculated.\n",
        "\n",
        "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
        "df.set_index(\"timestamp\", inplace=True)\n",
        "df = df.resample('15T').mean()"
      ],
      "metadata": {
        "id": "3HXyjUh-2haA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 8\n",
        "# Normalization\n",
        "df_normalized = (df - df.mean()) / df.std()"
      ],
      "metadata": {
        "id": "Mx4NDfrj2j-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 9\n",
        "# Smoothing (using 5-point moving average)\n",
        "df_smoothed = df.rolling(window=5).mean()"
      ],
      "metadata": {
        "id": "jQYMlB6v2o6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 10\n",
        "# Seasonal decomposition\n",
        "decomposition = seasonal_decompose(df[\"temperature\"], period=24)  # Assuming daily seasonality 24h\n",
        "trend = decomposition.trend\n",
        "seasonal = decomposition.seasonal\n",
        "residual = decomposition.resid"
      ],
      "metadata": {
        "id": "41n0r3pZ6juc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 - Time Series\n",
        "\n",
        "As this version of the octopus also collects temperature and humidity, lets have a look at the data!"
      ],
      "metadata": {
        "id": "xqqqrUFKYTrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 11\n",
        "# Plot Temperature\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df.index, df['temperature'], label='Temperature', color='red', marker='o')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('¬∞C')\n",
        "plt.title('First 10 Rows of Environmental Data')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8YTP039ivklv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 12\n",
        "# Plot Humidity\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df.index, df['humidity'], label='Humidity', color='blue', marker='o')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('%')\n",
        "plt.title('First 10 Rows of Environmental Data')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eJkTO6_4vovH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 12\n",
        "# Plot Barometric Pressure\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df.index, df['pressure'], label='Pressure', color='blue', marker='o')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('%')\n",
        "plt.title('First 10 Rows of Environmental Data')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rdyrFvPX-sOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3 - Mapping + Hotspot Analysis\n",
        "\n",
        "Now that we created some time series graphs, we will work on creating maps. Let's start with a PM2.5 map!"
      ],
      "metadata": {
        "id": "vJsfxTlowJUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Block 12\n",
        "#we're going to make a map. we start by setting a center point for the map to display the data\n",
        "coords = pmdata.loc[:,['latitude','longitude']].values #lat and lon are collected from CityScanner GPS\n",
        "start_point=coords[0]"
      ],
      "metadata": {
        "id": "EfoXm81NAviu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Block 13\n",
        "#here, we set up the specifications for the map\n",
        "newmap = folium.Map(location= start_point, tiles='Stamen Terrain', zoom_start=14)\n",
        "colormap = cm.LinearColormap(colors=['blue', 'green', 'yellow'], vmin=0, vmax=21)\n",
        "colormap.caption = 'PM 2.5 (ug/m^3)' #change this to represent the variable of interest!\n",
        "colormap.add_to(newmap)\n",
        "\n",
        "\n",
        "#this will loop through the data and show us where it's coming from\n",
        "#each point on the map will display the PM2.5 value from that spot, in micrograms per cubic meter.\n",
        "for i,row in pmdata.iterrows():\n",
        "    #folium.CircleMarker((row.latitude,row.longitude), radius=4, weight=1, color='blue', fill_color='blue', fill_opacity=.5, popup=(row.pm25)).add_to(newmap)\n",
        "    folium.CircleMarker((row.latitude,row.longitude), radius=4, weight=1, color=colormap(pmdata.iloc[i]['pm25']), fill ='true', fill_opacity=.5, popup=(row.pm25)).add_to(newmap)\n",
        "\n",
        "newmap.add_child(colormap)\n",
        "#here we save an html version of the map - you can zoom in and out of it and interact with it!\n",
        "#this .html file will be saved to your google drive folder. Download it to your computer and open it to interact with it!\n",
        "newmap.save('newmap.html')\n",
        "#you may need to refresh the page on your google drive folder to see the updated map!\n",
        "#you may also need to close a few tabs so you don't run out of memory when opening the map :)\n",
        "\n",
        "\n",
        "#note - you can also create a map for a subset of the total deployment time by calling the \"thdatamod\", \"pmdatamod\", or \"no2datamod\" variable above!\n"
      ],
      "metadata": {
        "id": "SmLF0qEhBpu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our point map, showing us where pm2.5 values and potential hotspot locations are, let's do some clustering. This will allow us to see where multiple measurements exceed the threshold value, potentially indicating a local source of pollution or pollution transport."
      ],
      "metadata": {
        "id": "9LjfKiQRBsDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Block 14\n",
        "#let's start by setting 10 as the threshold value. Change this and see how the number of hotspots changes!\n",
        "pmdata = pmdata.loc[(pmdata['pm25'] > 10)]"
      ],
      "metadata": {
        "id": "Kbh6fO9cBsvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Block 15\n",
        "#hierarchical clustering code\n",
        "\n",
        "# bottom-up hierarchical clustering - agglomerative, not k-means, because number of clusters not defined before\n",
        "hotspots = pmdata\n",
        "coords = hotspots.loc[:,['latitude','longitude']].values\n",
        "\n",
        "#preprocessing for hotspot clustering\n",
        "#we have to convert to radians, because scikit-learn‚Äôs haversine metric needs radian units\n",
        "kms_per_radian = 6371.0088\n",
        "\n",
        "#epsilon is the max distance points can be from each other to count as a cluster\n",
        "epsilon = 0.1 / kms_per_radian\n",
        "\n",
        "#min_samples is the minimum cluster size for a hotspot to be formed, and here we also call the haversine metric\n",
        "db = DBSCAN(eps=epsilon, min_samples=10, algorithm='ball_tree', metric='haversine').fit(np.radians(coords))\n",
        "cluster_labels = db.labels_\n",
        "num_clusters = len(set(cluster_labels))-(1 if -1 in set(cluster_labels) else 0)\n",
        "outliers = coords[cluster_labels == -1]\n",
        "\n",
        "#here is where we create the clusters after doing the background math above\n",
        "clusters = pd.Series([coords[cluster_labels == n] for n in range(num_clusters)])\n",
        "outliers = coords[cluster_labels == -1]\n",
        "print('Number of clusters: {}'.format(num_clusters))"
      ],
      "metadata": {
        "id": "Z0z_4cu1Buzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Block 16\n",
        "#this portion of the code is going to tell us where the map should show up!\n",
        "\n",
        "def get_centermost_point(cluster):\n",
        "    centroid = (MultiPoint(cluster).centroid.x, MultiPoint(cluster).centroid.y)\n",
        "    centermost_point = min(cluster, key=lambda point: great_circle(point, centroid).m)\n",
        "    return tuple(centermost_point)\n",
        "\n",
        "\n",
        "centermost_points = clusters.map(get_centermost_point)\n",
        "start_point=centermost_points[0]\n",
        "\n",
        "#setting up the specifications for the map\n",
        "hotspotmap = folium.Map(location= start_point, tiles='Stamen Terrain', zoom_start=14)\n",
        "points=[]\n",
        "#add a markers\n",
        "for index, row in hotspots.iterrows():\n",
        "    point=(row['latitude'] row['longitude'])\n",
        "    if point not in points:\n",
        "        new_point=(row['latitude'], row['longitude'])\n",
        "        points.append(new_point)\n",
        "for rep in centermost_points:\n",
        "    folium.CircleMarker(location=rep, color='blue', fill=True, fill_color='blue',radius=15).add_to(hotspotmap)\n",
        "for each in points:\n",
        "    folium.CircleMarker(location=each, popup=(row.pm25), color='red', fill=True, fill_color='red',radius=7).add_to(hotspotmap)\n",
        "    hotspotmap.add_child(folium.LatLngPopup())\n",
        "\n",
        "\n",
        "#interactive html map showing hotspot clusters\n",
        "hotspotmap.save('HotspotMap.html')"
      ],
      "metadata": {
        "id": "C_nsfb2OBxYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "Now that you have finished going through this notebook, you should be able to create time series graphs, basic maps, and perform clustering analysis techniques on hyperlocal environmental data. Additionally, you should have a basic knowledge of some of the different pollutants that can be measured to tell us more about the quality of our immediate environment.\n",
        "\n",
        "\n",
        "## Reference Materials:\n",
        "\n",
        "[NYCCAS Data](https://nyc-ehs.net/nyccas2020/web/report#Pollutant_Maps)\n",
        "\n",
        "[United States EPA](https://www.epa.gov/)\n",
        "\n",
        "[World Health Organization](https://www.who.int/)\n",
        "\n",
        "\n",
        "\n",
        "## Python Library Documentation:\n",
        "\n",
        "[Folium](https://python-visualization.github.io/folium/latest/)\n",
        "\n",
        "[Pandas](https://pandas.pydata.org/)\n",
        "\n",
        "[Matplotlib](https://matplotlib.org/)"
      ],
      "metadata": {
        "id": "nZwB05vxB_1w"
      }
    }
  ]
}